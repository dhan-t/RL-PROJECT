# üö® CRITICAL BUG FIX - Action Space Mismatch

## **Problem Identified**

After `git pull` from your groupmate's changes, the RL agents stopped working properly. The GUI showed "Actor-Critic Score: 1" instead of the expected 85-95+ scores.

### **Root Cause: Action Space Reduction**

Your groupmate **removed the "No Action" option**, reducing the action space from 3 actions to 2 actions:

**BEFORE (Working):**
- Action 0: Add Carriage (+100 capacity, -10 cost)
- Action 1: Widen Carriage (+50 capacity, -5 cost)
- **Action 2: No Action (0 cost)** ‚úÖ **OPTIMAL STRATEGY**

**AFTER (Broken):**
- Action 0: Add Carriage (+100 capacity, -10 cost)
- Action 1: Widen Carriage (+50 capacity, -5 cost)
- ~~Action 2: No Action~~ ‚ùå **REMOVED**

### **Why This Broke Everything**

1. **Your original agents learned that "No Action" was the optimal strategy** (~91% of the time)
2. **Removing this option forced agents to waste money on every step**
3. **The saved agent models (.pt, .pkl files) were trained on action_dim=2**, incompatible with the original 3-action strategy
4. **Environment validation failed** - trying to select action 2 raised `ValueError`

---

## **What Was Fixed**

### ‚úÖ **1. Environment (`train_game_env.py`)**
```python
# BEFORE
self.action_space = spaces.Discrete(2)  # Only Add, Widen

# AFTER  
self.action_space = spaces.Discrete(3)  # Add, Widen, No Action
```

Added back the "No Action" (action=2) case in the `step()` method:
```python
elif action == 2:  # No action (do nothing)
    cost, weight = 0.0, 0.0
```

### ‚úÖ **2. Training Notebook (`rl_training.ipynb`)**

**Updated all agent instantiations:**
```python
# BEFORE
mc_agent = MonteCarloAgent(n_actions=2, eps=0.1)
q_agent = QLearningAgent(n_actions=2, alpha=0.1, gamma=0.99, eps=0.1)
ac_agent = ActorCriticAgent(state_dim=6, action_dim=2, lr=1e-3, gamma=0.99)

# AFTER
mc_agent = MonteCarloAgent(n_actions=3, eps=0.1)
q_agent = QLearningAgent(n_actions=3, alpha=0.1, gamma=0.99, eps=0.1)
ac_agent = ActorCriticAgent(state_dim=6, action_dim=3, lr=1e-3, gamma=0.99)
```

**Restored intelligent default action:**
```python
# When no Q-values learned yet, prefer no-action (2) instead of random
if all(q == 0 for q in qvals):
    return 2  # Prefer no action initially
```

### ‚úÖ **3. GUI Game (`gui_train_game.py`)**

**Updated all agent classes to support 3 actions:**
- `ActorCriticNet`: `action_dim=3`
- `ActorCriticAgentWrapper`: `action_dim=3`
- `MonteCarloAgent`: `n_actions=3`
- `QLearningAgent`: `n_actions=3`
- `SimpleRuleAgent`: `n_actions=3` (fallback returns action 2)

---

## **Why "No Action" Is Critical**

### üéØ **Reward Structure Analysis**

The reward at each step is:
```python
reward = (passengers_boarded √ó 1.5) - (unused_capacity √ó 0.5) - config_cost
```

**Example at Step 1:**
- **With "No Action"**: 50 board, 50 unused ‚Üí `(50√ó1.5) - (50√ó0.5) - 0 = 50` reward
- **With "Add Carriage"**: 50 board, 50 unused ‚Üí `(50√ó1.5) - (150√ó0.5) - 10 = -10` reward ‚ùå

**Your agent discovered:**
- Starting capacity (100) is often sufficient
- **Boarding passengers generates +1.5 reward/passenger**
- **Adding capacity costs -10 immediately + ongoing waste penalties**
- **Optimal strategy: Do nothing, maximize boarding with existing capacity**

This is why your original agent achieved 91.4/100 with **0.0 config cost** - it never modified the train!

---

## **Next Steps**

### üîÑ **You Need to Retrain**

The old saved models (`saved_agents/*.pt`, `saved_agents/*.pkl`) were trained with `action_dim=2` and are **incompatible**. You need to:

1. **Delete old saved agents:**
   ```bash
   rm saved_agents/*.pt saved_agents/*.pkl
   ```

2. **Run training cells again** (Cells 3-6 in notebook)
   - This will take ~30 minutes for 500 episodes
   - Expected results:
     - Actor-Critic: **85-95/100** (will learn "No Action" strategy)
     - Q-Learning: **70-80/100**
     - Monte Carlo: **15-25/100** (struggles with large state space)

3. **Save the new models** (Cell 6)

4. **Test in GUI** - should now show proper scores

---

## **Verification Checklist**

Before retraining, verify these changes:

- [ ] `train_game_env.py` line 44: `spaces.Discrete(3)`
- [ ] `train_game_env.py` line 240: `elif action == 2:` case exists
- [ ] `rl_training.ipynb` Cell 3: All agents use `n_actions=3`
- [ ] `rl_training.ipynb` Cell 5: Agent instantiation uses 3 actions
- [ ] `gui_train_game.py` line 46: `action_dim=3`
- [ ] `gui_train_game.py` line 68: `action_dim=3`
- [ ] All agent classes in GUI have `n_actions=3`

---

## **Lesson Learned**

**‚ö†Ô∏è Always maintain action space consistency throughout the system:**
- Environment action space
- Agent architecture (network output dimensions)
- Saved models
- Training code
- Inference/deployment code

**Changing the action space = retraining from scratch!**

---

## **Expected Training Results**

After retraining with 3-action system, you should see:

```
Actor-Critic:
  Average Score: 90-92/100 ‚úÖ
  Config Cost: ~0.0 (will learn "No Action" strategy)
  
Q-Learning:
  Average Score: 75-80/100 ‚úÖ
  Config Cost: ~20-50
  
Monte Carlo:
  Average Score: 20-30/100 (state space too large)
  Config Cost: Variable
```

The **Actor-Critic agent will dominate** by learning to:
1. Start with 100 capacity
2. Choose "No Action" (~90% of steps)
3. Only add/widen during extreme peak hours
4. Maximize passenger boarding while minimizing costs

**This is the correct, optimal strategy!** üéâ
