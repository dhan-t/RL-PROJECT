# ğŸš† Train Capacity Management with Reinforcement Learning# RL PROJECT: Train Capacity Management Game



**A comprehensive RL project demonstrating Monte Carlo, Q-Learning, and Actor-Critic algorithms on a real-world transit optimization problem.**A Gymnasium-compliant reinforcement learning environment for training agents to manage train capacity in a realistic transit system simulation.



[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)## ğŸ¯ Overview

[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)

[![Gymnasium](https://img.shields.io/badge/Gymnasium-0.26+-green.svg)](https://gymnasium.farama.org/)This project implements a train capacity management game based on Manila's LRT-2 line, where RL agents learn to balance:

- **Passenger boarding** (maximize)

---- **Capacity efficiency** (minimize waste)

- **Infrastructure stress** (prevent collapse)

## ğŸ“‹ Table of Contents- **Configuration costs** (minimize expansions)



- [Overview](#-overview)## âœ… Fixed Issues (Comprehensive Review)

- [Key Results](#-key-results)

- [Quick Start](#-quick-start)### **Critical Fixes Applied:**

- [Project Structure](#-project-structure)

- [Documentation](#-documentation)1. **âœ… Gymnasium API Compliance**

- [Visualization](#-visualization)   - `step()` now returns 5-tuple: `(obs, reward, terminated, truncated, info)`

- [TensorBoard](#-tensorboard-integration)   - `reset()` now returns 2-tuple: `(obs, info)`

- [Future Work](#-future-work)   - Action and observation spaces are attributes (not properties)

   - Added proper `render_mode` support

---

2. **âœ… Reward Double-Counting Bug Fixed**

## ğŸ¯ Overview   - Configuration cost was being subtracted twice

   - Now applied only once in `step_reward` calculation

This project implements a **Gymnasium-compliant environment** simulating Manila's LRT-2 train system, where RL agents learn optimal capacity management strategies across 13 stations with dynamic passenger demand.

3. **âœ… Consistent State Space**

### Environment Features   - 6-dimensional state: `[capacity, passengers, station_idx, direction, hour, minute]`

   - All files updated to use the same dimensions

- **State Space:** 6D continuous (capacity, passengers, station, direction, time)

- **Action Space:** 3 discrete actions (Add Carriage +100, Widen +50, No Action)4. **âœ… Info Dictionary Consistency**

- **Reward Structure:** Balance boarding, efficiency, costs, and penalties   - Standardized structure across all code paths

- **Realistic Simulation:** Time-of-day multipliers, station types, capacity decay   - Includes: alighted, boarded, arrivals, penalties, efficiency metrics



### Algorithms Implemented5. **âœ… Agent Implementations Updated**

   - Fixed Gymnasium API compatibility in all training loops

1. **Monte Carlo** - First-visit MC with epsilon-greedy exploration   - Updated Q-Learning, Monte Carlo, and Actor-Critic agents

2. **Q-Learning** - Temporal-difference learning with decay schedules

3. **Actor-Critic** - Policy gradient with baseline (PyTorch neural network)## ğŸš€ Quick Start



---### Prerequisites



## ğŸ† Key Results```bash

# Activate the conda environment with gymnasium

**Training:** 1000 episodes per agent | **Evaluation:** 100 episodes | **Seed:** 42conda activate pt1

```

| Agent | Score | Consistency | Efficiency | Strategy |

|-------|-------|-------------|------------|----------|### Basic Usage

| **Actor-Critic** ğŸ¥‡ | **56.9 Â± 4.0** | â­â­â­â­â­ | **568.70** | 100% No Action |

| **Q-Learning** ğŸ¥ˆ | **55.4 Â± 42.3** | â­â­ | **1.13** | 71% No Action |```python

| **Monte Carlo** ğŸ¥‰ | **41.6 Â± 43.1** | â­ | **0.68** | 61% No Action |from train_game_env import TrainGameEnv



### Key Findings# Create environment

env = TrainGameEnv(initial_capacity=100, seed=42)

âœ… **Actor-Critic achieved optimal convergence** - discovered "No Action" as mathematically optimal  

âœ… **Policy gradients converge faster** - 600 episodes vs 800+ for value-based methods  # Reset environment

âœ… **Low variance indicates convergence** - Actor-Critic Â±4.0 vs Q-Learning Â±42.3  obs, info = env.reset()

âœ… **Game design impacts strategy** - current parameters favor conservative play  

# Take a step

ğŸ“„ **Detailed analysis:** [ANALYSIS_REPORT.md](./ANALYSIS_REPORT.md)action = env.action_space.sample()  # or 0, 1, 2

obs, reward, terminated, truncated, info = env.step(action)

---

# Get final score

## ğŸš€ Quick Startif terminated or truncated:

    normalized_score, raw_score = env.final_score()

### Installation    print(f"Final Score: {normalized_score}/100")

```

```bash

# Clone repository## ğŸ“Š Environment Specification

git clone https://github.com/yourusername/RL-PROJECT.git

cd RL-PROJECT### Action Space

- **Type:** `Discrete(3)`

# Activate conda environment- **Actions:**

conda activate pt1  - `0`: Add carriage (+100 capacity, cost=10, weight=1.0)

  - `1`: Widen carriage (+50 capacity, cost=5, weight=0.5)

# Install dependencies (if needed)  - `2`: No action (cost=0, weight=0)

conda install pytorch numpy matplotlib gymnasium

```### Observation Space

- **Type:** `Box(6,)` with dtype `float32`

### Train Agents- **Dimensions:**

  1. `capacity`: Current train capacity [0, âˆ)

```bash  2. `passengers_onboard`: Current passengers [0, âˆ)

cd scripts  3. `station_idx`: Current station [0, 12]

  4. `direction`: Travel direction {-1, 1}

# Train all agents (recommended)  5. `hour`: Current hour [0, 23]

python train.py --agent all --episodes 1000  6. `minute`: Current minute [0, 59]



# Train specific agent### Reward Structure

python train.py --agent qlearning --episodes 1000

```python

# Train with TensorBoard loggingreward = boarding_reward - efficiency_penalty - config_penalty

python train_with_tensorboard.py --agent all --episodes 1000

```where:

  boarding_reward = 1.5 Ã— passengers_boarded

### Evaluate Performance  efficiency_penalty = calculated based on unused capacity and time

  config_penalty = 2.0 Ã— action_cost

```bash```

# Evaluate all agents

python evaluate.py --all --episodes 100### Termination Conditions

- **Terminated:** Infrastructure collapse or end of operating hours (22:00)

# Evaluate specific agent- **Truncated:** Maximum steps reached (2000)

python evaluate.py --agent actorcritic --episodes 100

```## ğŸ¤– Training RL Agents



### Visualize ResultsThe project includes three RL algorithms:



```bash1. **Monte Carlo** (tabular, first-visit)

# Generate all plots2. **Q-Learning** (tabular, off-policy)

python visualize.py --mode all3. **Actor-Critic** (neural network, policy gradient)



# Agent comparison only### Running Training

python visualize.py --mode comparison

Open `rl_training.ipynb` in Jupyter:

# Training curves only

python visualize.py --mode training```bash

conda activate pt1

# Interactive mode (show plots)jupyter notebook rl_training.ipynb

python visualize.py --mode all --show```

```

Or run cells directly to train agents.

### Watch Agents Play

## ğŸ“ Project Structure

```bash

# GUI demo (recommended)```

python play_demo_gui.pyRL-PROJECT/

â”œâ”€â”€ train_game_env.py       # Main Gymnasium environment

# Terminal demoâ”œâ”€â”€ rl_training.ipynb        # Agent training notebook

python play_demo.py --agent actorcritic --episodes 3â”œâ”€â”€ GAME.ipynb               # Game development history

```â”œâ”€â”€ SCRAP_NB.ipynb          # Experimental code

â”œâ”€â”€ ISSUES_AND_FIXES.md     # Comprehensive issue analysis

---â””â”€â”€ README.md               # This file

```

## ğŸ“ Project Structure

## ğŸ”§ Key Design Decisions

```

RL-PROJECT/### Why These Fixes Matter

â”œâ”€â”€ ğŸ“„ ANALYSIS_REPORT.md          # Comprehensive results analysis

â”œâ”€â”€ ğŸ“„ REBALANCING_GUIDE.md        # Future game design improvements1. **Gymnasium Compliance**: Enables use with Stable-Baselines3, RLlib, and other standard RL libraries

â”œâ”€â”€ ğŸ“„ VISUALIZATION_GUIDE.md      # Plotting quickstart2. **Reward Fixes**: Ensures agents can actually learn (previous double-penalty made learning nearly impossible)

â”œâ”€â”€ ğŸ“„ GAME_BALANCE_GUIDE.md       # Game mechanics documentation3. **State Consistency**: Prevents crashes and confusion across different code versions

â”‚4. **Proper API**: Follows RL community standards for environment design

â”œâ”€â”€ scripts/                        # Main codebase

â”‚   â”œâ”€â”€ agents.py                   # RL algorithm implementations### Reward Design Philosophy

â”‚   â”œâ”€â”€ config.py                   # Hyperparameters & constants

â”‚   â”œâ”€â”€ train.py                    # Training pipelineThe reward structure balances:

â”‚   â”œâ”€â”€ train_with_tensorboard.py   # Training with TB logging- **Positive reinforcement** for serving passengers

â”‚   â”œâ”€â”€ evaluate.py                 # Evaluation framework- **Negative feedback** for inefficiency (unused capacity)

â”‚   â”œâ”€â”€ visualize.py                # Matplotlib plotting- **Cost-awareness** for infrastructure expansion

â”‚   â”œâ”€â”€ play_demo.py                # Terminal demo- **Time-varying difficulty** to prevent exploitation

â”‚   â”œâ”€â”€ play_demo_gui.py            # GUI launcher

â”‚   â”œâ”€â”€ viz_guide.py                # Visualization guide## ğŸ“ˆ Expected Performance

â”‚   â””â”€â”€ README.md                   # Scripts documentation

â”‚With proper training (500+ episodes):

â”œâ”€â”€ train_game_env.py               # Gymnasium environment- **Random Agent:** ~20-30/100

â”œâ”€â”€ gui_train_game.py               # Standalone GUI application- **Monte Carlo:** ~40-55/100

â”œâ”€â”€ test_3action_system.py          # Environment tests- **Q-Learning:** ~45-60/100

â”‚- **Actor-Critic:** ~50-70/100 (best)

â”œâ”€â”€ saved_agents/                   # Trained model checkpoints

â”‚   â”œâ”€â”€ monte_carlo_model.pkl## ğŸ› Known Limitations

â”‚   â”œâ”€â”€ q_learning_model.pkl

â”‚   â””â”€â”€ actor_critic_best_model.pt1. **State space explosion** for tabular methods (discretization needed)

â”‚2. **Long training times** for Actor-Critic (neural network)

â”œâ”€â”€ visualizations/                 # Generated plots3. **Sparse reward signal** in some scenarios

â”‚   â”œâ”€â”€ training_curves.png4. **Stochastic environment** makes learning challenging

â”‚   â””â”€â”€ agent_comparison.png

â”‚## ğŸ”¬ Future Improvements

â””â”€â”€ runs/                           # TensorBoard logs

    â””â”€â”€ train_YYYYMMDD-HHMMSS/- [ ] Add reward normalization wrapper

```- [ ] Implement PPO/SAC for better performance

- [ ] Add action masking for invalid actions

---- [ ] Create vectorized environment for parallel training

- [ ] Add more sophisticated state discretization

## ğŸ“š Documentation- [ ] Implement curriculum learning



### Core Documents## ğŸ“š References



- **[ANALYSIS_REPORT.md](./ANALYSIS_REPORT.md)** - Complete results, insights, and research findings- **Gymnasium Documentation:** https://gymnasium.farama.org/

- **[REBALANCING_GUIDE.md](./REBALANCING_GUIDE.md)** - How to modify game for more complex strategies- **Stable-Baselines3:** https://stable-baselines3.readthedocs.io/

- **[VISUALIZATION_GUIDE.md](./VISUALIZATION_GUIDE.md)** - Plotting quick start and customization- **Sutton & Barto:** Reinforcement Learning: An Introduction

- **[scripts/README.md](./scripts/README.md)** - API reference and command guide

## ğŸ¤ Contributing

### Guides

This is an educational project. Feel free to experiment with:

- **[GAME_BALANCE_GUIDE.md](./GAME_BALANCE_GUIDE.md)** - Game mechanics and parameter tuning- Different reward structures

- **[RETRAINING_GUIDE.md](./RETRAINING_GUIDE.md)** - How to retrain after changes- New RL algorithms

- Environment modifications

---- Better state representations



## ğŸ“Š Visualization## ğŸ“ License



### Generated PlotsEducational use only.



**Training Curves Dashboard** (`training_curves.png`):---

- Learning curves (50-episode moving average)

- Config costs over time**Last Updated:** October 11, 2025  

- Reward progression**Status:** âœ… All critical issues fixed and tested  

- Score distributions**Environment:** Fully Gymnasium-compliant

- Early vs late performance

- Efficiency metrics (score/cost)

- Variance analysis

A Gymnasium-compatible reinforcement learning environment for train capacity optimization, simulating the LRT-2 line in Metro Manila.

**Agent Comparison** (`agent_comparison.png`):

- Average scores with error bars## ğŸ¯ Overview

- Configuration cost comparison

- Score distribution violin plotsThis project implements a realistic train capacity management simulation where RL agents must balance:

- Action usage breakdown (stacked bars)- **Passenger boarding** (reward for serving passengers)

- **Capacity efficiency** (penalty for unused space)

### Generate Plots- **Infrastructure stress** (risk of collapse from overbuilding)

- **Configuration costs** (penalty for capacity changes)

```bash

# Quick generation## ğŸ—ï¸ Project Structure

python scripts/visualize.py --mode all

```

# Custom output directoryRL-PROJECT/

python scripts/visualize.py --mode all --output-dir ./my_plotsâ”œâ”€â”€ train_game_env.py      # Gymnasium-compliant environment

â”œâ”€â”€ rl_training.ipynb      # Agent implementations & training

# Interactive viewerâ”œâ”€â”€ test_environment.py    # Unit tests for environment

python scripts/visualize.py --mode comparison --showâ”œâ”€â”€ ISSUES_AND_FIXES.md    # Detailed analysis of bugs found

â”œâ”€â”€ GAME.ipynb             # Legacy versions (for reference)

# View guideâ””â”€â”€ README.md              # This file

python scripts/viz_guide.py```

```

## ğŸš€ Quick Start

---

### Installation

## ğŸ”¥ TensorBoard Integration

```bash

### Real-Time Monitoring# Install dependencies

pip install gymnasium torch matplotlib numpy

```bash```

# Terminal 1: Train with TensorBoard logging

cd scripts### Running Tests

python train_with_tensorboard.py --agent all --episodes 1000

```bash

# Terminal 2: Start TensorBoard# Test environment correctness

tensorboard --logdir=../runspython test_environment.py

```

# Open browser: http://localhost:6006

```### Training Agents



### What TensorBoard ShowsOpen `rl_training.ipynb` in Jupyter and run all cells to:

1. Test environment compatibility

**Scalars:**2. Load improved agent implementations

- Episode scores (per agent)3. Train Monte Carlo, Q-Learning, and Actor-Critic agents

- Configuration costs4. Compare performance with visualizations

- Episode rewards

- Exploration parameters (epsilon, alpha)## ğŸ® Environment Details

- Loss functions (Actor-Critic)

- Moving averages (50-episode window)### State Space (6D)

- Standard deviations- `capacity`: Current train capacity [0, âˆ)

- Comparison metrics- `passengers_onboard`: Current passenger count [0, capacity]

- `station_idx`: Current station [0-12]

**Graphs:**- `direction`: Travel direction {-1, 1}

- Network architecture (Actor-Critic)- `hour`: Current hour [0-23]

- Computational graph- `minute`: Current minute [0-59]



**Projector:**### Action Space (Discrete, 3 actions)

- State space embeddings (optional)- `0`: Add carriage (+100 capacity, cost=10, weight=1.0)

- `1`: Widen carriage (+50 capacity, cost=5, weight=0.5)

---- `2`: No action (cost=0, weight=0)



## ğŸ”¬ Research Insights### Reward Structure

```python

### Exploration vs Exploitationreward = (1.5 Ã— boarded) - efficiency_penalty - (2.0 Ã— cost)

```

```

Monte Carlo:  High exploration (Îµ=0.01) â†’ Slow convergence â†’ High variance### Episode Termination

Q-Learning:   Balanced (Îµ-greedy)      â†’ Moderate speed  â†’ Moderate variance  - **Terminated**: Infrastructure collapse or operating hours end (22:00)

Actor-Critic: Fast exploitation        â†’ Fast convergence â†’ Low variance- **Truncated**: Maximum steps reached (2000)

```

## ğŸ¤– Agent Implementations

### Convergence Patterns

### 1. Monte Carlo

```- First-visit Monte Carlo with epsilon-greedy policy

Episodes to Stable Policy:- Bounded state discretization

â”œâ”€â”€ Actor-Critic: ~600 (100% No Action strategy)- Epsilon decay for exploration-exploitation balance

â”œâ”€â”€ Q-Learning:   ~800 (70% No Action, still exploring)

â””â”€â”€ Monte Carlo:  >1000 (60% No Action, high exploration)### 2. Q-Learning

```- Off-policy TD learning

- Learning rate decay

### Variance-Performance Tradeoff- Epsilon-greedy exploration



| Metric | Indicates |### 3. Actor-Critic

|--------|-----------|- Policy gradient with value function baseline

| Low variance (Â±4) | Policy converged (deterministic) |- Entropy regularization for exploration

| High variance (Â±40) | Still exploring (stochastic) |- Normalized returns for stable learning



---## âœ… Fixes Applied



## ğŸ“ Educational Value### Critical Fixes

1. **Gymnasium API Compliance**

### What Students Learn   - âœ… `reset()` returns `(observation, info)`

   - âœ… `step()` returns `(obs, reward, terminated, truncated, info)`

1. **RL Fundamentals**   - âœ… Spaces are attributes, not properties

   - Value-based vs policy-based methods

   - Exploration-exploitation tradeoff2. **Reward Double-Counting Bug**

   - Convergence analysis   - âœ… Configuration cost no longer subtracted twice

   - âœ… Consistent reward calculation

2. **Deep RL**

   - Policy gradient methods3. **State Space Issues**

   - Actor-Critic architecture   - âœ… Bounded discretization (prevents infinite state space)

   - Neural network optimization   - âœ… Consistent 6D state across all files

   - âœ… Proper normalization for neural networks

3. **Practical Skills**

   - Gymnasium API usage4. **Agent Improvements**

   - PyTorch implementation   - âœ… Epsilon and learning rate decay

   - Hyperparameter tuning   - âœ… Better Actor-Critic architecture

   - Performance evaluation   - âœ… Entropy regularization

   - âœ… Gradient clipping

4. **Game Design**

   - Reward shaping5. **Info Dictionary**

   - Action space design   - âœ… Consistent structure across all code paths

   - Optimal strategy analysis   - âœ… Comprehensive metrics for debugging



---See `ISSUES_AND_FIXES.md` for detailed analysis.



## ğŸ”® Future Work## ğŸ“Š Expected Results



### Option 1: Accept Current Results âœ…After training (500 episodes):

- **Monte Carlo**: ~45-55 score

**Status:** **RECOMMENDED** - Current implementation is complete- **Q-Learning**: ~50-60 score  

- **Actor-Critic**: ~55-70 score

- Document findings (âœ… Done: [ANALYSIS_REPORT.md](./ANALYSIS_REPORT.md))

- Publish resultsResults may vary due to stochastic environment.

- Use as baseline for comparisons

- Educational demonstration## ğŸ§ª Testing



### Option 2: Rebalance Game ğŸ®The `test_environment.py` file includes comprehensive tests:

- Gymnasium API compliance

**Status:** Proposed - See [REBALANCING_GUIDE.md](./REBALANCING_GUIDE.md)- Seeding reproducibility

- State validity

**Quick changes for action diversity:**- Action effects

```python- Info dictionary consistency

# train_game_env.py- Episode completion

initial_capacity = 10  # Force expansion (was 25)- No double-counting verification

capacity_match_bonus = 100.0 * rush_hour_multiplier  # Incentivize optimization (was 25.0)

penalty_missed = 10.0 * (missed_passengers ** 1.5)  # Punish shortfalls (was 3.0 Ã— ^1.2)## ğŸ“ Usage Example

```

```python

**Expected outcome:** 40-60% action usage, more strategic gameplayfrom train_game_env import TrainGameEnv



---# Create environment

env = TrainGameEnv(seed=42)

## ğŸ§ª Experimental Reproducibility

# Reset

### Reproduction Stepsobs, info = env.reset()



```bash# Run episode

# 1. Clone repositorydone = False

git clone https://github.com/yourusername/RL-PROJECT.gitwhile not done:

cd RL-PROJECT    action = env.action_space.sample()

    obs, reward, terminated, truncated, info = env.step(action)

# 2. Setup environment    done = terminated or truncated

conda activate pt1

# Get final score

# 3. Train (uses fixed seed=42)score, raw_score = env.final_score()

cd scriptsprint(f"Score: {score}/100")

python train.py --agent all --episodes 1000```



# 4. Evaluate## ğŸ”§ Customization

python evaluate.py --all --episodes 100

Adjust hyperparameters in environment:

# 5. Compare results with ANALYSIS_REPORT.md```python

```env = TrainGameEnv(

    initial_capacity=100,      # Starting capacity

### Key Parameters    seed=42,                   # Random seed

    verbose=False,             # Print debug info

- **Random Seed:** 42 (fixed for reproducibility)    render_mode='human'        # Enable rendering

- **Episodes:** 1000 training, 100 evaluation)

- **Initial Capacity:** 25```

- **Action Costs:** Add=2.0, Widen=1.0, NoAction=0.0

- **Hyperparameters:** See `scripts/config.py`## ğŸ“š References



---- [Gymnasium Documentation](https://gymnasium.farama.org/)

- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/)

## ğŸ› ï¸ Troubleshooting- Sutton & Barto - Reinforcement Learning: An Introduction



### Common Issues## ğŸ¤ Contributing



**"ModuleNotFoundError: No module named 'gymnasium'"**This is a learning project. Feel free to experiment with:

```bash- Different reward structures

conda install gymnasium- New agent algorithms (PPO, DQN, A3C)

```- Environment variations

- Better visualization

**"Import torch could not be resolved"**

```bash## ğŸ“„ License

conda install pytorch

```Educational project - free to use and modify.



**"No trained agents found"**---

```bash

cd scripts**Last Updated**: October 11, 2025

python train.py --agent all --episodes 1000
```

**"KeyError: 'final_score'"**
- Fixed in current version
- Use `env.final_score()` method, not info dict

**TensorBoard not showing data**
```bash
# Check log directory
ls -la ../runs/

# Restart TensorBoard
tensorboard --logdir=../runs --reload_interval=5
```

---

## ğŸ“ Citation

If you use this project in your research, please cite:

```bibtex
@misc{train-capacity-rl-2025,
  author = {Your Name},
  title = {Train Capacity Management with Reinforcement Learning},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/yourusername/RL-PROJECT}
}
```

---

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:

- Additional RL algorithms (PPO, SAC, DQN)
- Real-world ridership data integration
- Multi-agent scenarios
- Transfer learning experiments
- Alternative reward structures

---

## ğŸ“„ License

This project is open source and available under the MIT License.

---

## ğŸ‘¥ Credits

**Environment:** LRT-2 Manila Metro (13 stations)  
**Algorithms:** Monte Carlo, Q-Learning, Actor-Critic  
**Framework:** PyTorch + Gymnasium  
**Visualization:** Matplotlib + TensorBoard

---

## ğŸ“ Contact

Questions? Open an issue or reach out!

---

**Status:** âœ… **Complete - Results Accepted**  
**Last Updated:** October 18, 2025  
**Version:** 1.0

*Happy training! ğŸš‚*
